{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar dados do Lakehouse Bronze para Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho ou referência da tabela Bronze (tabela \"produto\" no Lakehouse Bronze)\n",
    "\n",
    "bronze_table = \"Bronze_Lakehouse_GL.produto\"  # Nome da tabela no Lakehouse Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados da camada Bronze\n",
    "df_bronze = spark.read.format(\"delta\").table(bronze_table)  # Usando .table para ler diretamente de Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir o schema para garantir que os dados foram carregados corretamente\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformações no DataFrame (opcional)\n",
    "\n",
    "O objetivo da transformação para a camada Silver é:\n",
    "\n",
    "**Limpeza de Dados:** Eliminar dados inválidos, nulos ou duplicados.\n",
    "\n",
    "**Estruturação:** Organizar os dados de uma forma mais estruturada, como a separação de campos compostos ou a conversão de tipos de dados.\n",
    "\n",
    "**Enriquecimento Básico:** Você pode adicionar dados adicionais, como a conversão de códigos em valores legíveis ou a adição de metadados para melhor compreensão dos dados.\n",
    "\n",
    "**Normalização:** Ajustar os dados para garantir que tenham formatos e unidades consistentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar transformações, se necessário\n",
    "## Exemplo:  df_transformed = df_bronze.select(\"column1\", \"column2\")  # Exemplo de transformação\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Conversão de tipos de dados\n",
    "df_transformed = df_bronze.withColumn(\"CUSTO_MEDIO\", col(\"CUSTO_MEDIO\").cast(\"double\")) \\\n",
    "                          .withColumn(\"VALOR_UNITARIO\", col(\"VALOR_UNITARIO\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, expr\n",
    "\n",
    "# Tratar valores nulos\n",
    "df_transformed = df_transformed.fillna({\n",
    "    \"IDSK\": \"Desconhecido\",\n",
    "    \"IDPRODUTO\": \"Desconhecido\",\n",
    "    \"NOME\": \"Sem Nome\",\n",
    "    \"ID_CATEGORIA\": \"Desconhecida\",\n",
    "    \"CUSTO_MEDIO\": 0.0,\n",
    "    \"VALOR_UNITARIO\": 0.0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, expr\n",
    "\n",
    "# Padronização de texto\n",
    "df_transformed = df_transformed.withColumn(\"NOME\", upper(col(\"NOME\"))) \\\n",
    "                               .withColumn(\"ID_CATEGORIA\", upper(col(\"ID_CATEGORIA\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, expr\n",
    "\n",
    "\n",
    "# Criar coluna derivada: Margem Bruta\n",
    "df_transformed = df_transformed.withColumn(\"MARGEM_BRUTA\", expr(\"VALOR_UNITARIO - CUSTO_MEDIO\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, expr\n",
    "\n",
    "\n",
    "# Remover duplicatas\n",
    "df_transformed = df_transformed.dropDuplicates([\"IDPRODUTO\", \"IDSK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, expr\n",
    "\n",
    "# Validar valores numéricos\n",
    "df_transformed = df_transformed.filter((col(\"CUSTO_MEDIO\") >= 0) & (col(\"VALOR_UNITARIO\") >= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, expr\n",
    "\n",
    "# Exibir esquema e algumas linhas para verificar\n",
    "df_transformed.printSchema()\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar no Lakehouse Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o caminho ou referência da tabela Silver\n",
    "silver_table = \"Silver_Lakehouse_GL.produto\"  # Nome da tabela no Lakehouse Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravar os dados na camada Silver\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)  # Salva a tabela no formato Delta"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
